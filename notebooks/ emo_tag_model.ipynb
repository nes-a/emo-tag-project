{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN1aigsGuscQHjKaX4l6zvA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nes-a/emo-tag-project/blob/main/notebooks/%20emo_tag_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install --upgrade datasets\n",
        "!pip install wandb\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "LW7KrXDcDDb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "from datasets import Dataset\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding\n",
        "import wandb\n",
        "wandb.login()\n",
        "import os\n",
        "# Keep if you intend to use wandb; remove/set to \"true\" if not.\n",
        "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
        "\n",
        "print(\"All installations and imports processed.\")"
      ],
      "metadata": {
        "id": "MCrTkSGcdqzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep if you intend to use\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gVnfC17xds3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Constants and Mappings ---\n",
        "GOEMOTIONS_LABELS = [\n",
        "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
        "    'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n",
        "    'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',\n",
        "    'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
        "]\n",
        "\n",
        "GOEMOTIONS_ID_TO_LABEL = {\n",
        "    0: 'admiration', 1: 'amusement', 2: 'anger', 3: 'annoyance', 4: 'approval', 5: 'caring', 6: 'confusion',\n",
        "    7: 'curiosity', 8: 'desire', 9: 'disappointment', 10: 'disapproval', 11: 'disgust', 12: 'embarrassment',\n",
        "    13: 'excitement', 14: 'fear', 15: 'gratitude', 16: 'grief', 17: 'joy', 18: 'love', 19: 'nervousness',\n",
        "    20: 'optimism', 21: 'pride', 22: 'realization', 23: 'relief', 24: 'remorse', 25: 'sadness', 26: 'surprise'\n",
        "}"
      ],
      "metadata": {
        "id": "AfU8GMbdd1k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Parsing and Loading Functions ---\n",
        "def parse_synthetic_emotion_string(emotion_str):\n",
        "    \"\"\"\n",
        "    Parses a string representation of a list of emotions into an actual list.\n",
        "    Handles potential 'neutral' or empty lists, and single string emotions.\n",
        "    \"\"\"\n",
        "    if pd.isna(emotion_str):\n",
        "        return ['neutral']\n",
        "\n",
        "    try:\n",
        "        parsed_list = ast.literal_eval(emotion_str)\n",
        "        if isinstance(parsed_list, list):\n",
        "            return parsed_list if parsed_list else ['neutral']\n",
        "        else:\n",
        "            return [str(parsed_list)] if str(parsed_list).strip() else ['neutral']\n",
        "    except (ValueError, SyntaxError):\n",
        "\n",
        "        if emotion_str.strip():\n",
        "            return [emotion_str.strip()]\n",
        "        else:\n",
        "            return ['neutral']\n"
      ],
      "metadata": {
        "id": "RlhN8GO3GwwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_go_emotions_split(filepath):\n",
        "\n",
        "    \"\"\"\n",
        "    Loads and processes a GoEmotions TSV split, assigning 'neutral' where no other emotions are found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df_split = pd.read_csv(filepath, sep='\\t', encoding='utf-8', header=None, names=['text', 'emotion_ids_str', 'comment_id'])\n",
        "        print(f\"Loaded {filepath.split('/')[-1]}. Initial shape: {df_split.shape}\")\n",
        "        print(f\"Columns in {filepath.split('/')[-1]} after initial load: {df_split.columns.tolist()}\")\n",
        "\n",
        "        df_split['emotion_ids_list'] = df_split['emotion_ids_str'].apply(\n",
        "            lambda x: [int(label_id) for label_id in str(x).split(',') if label_id.strip().isdigit()]\n",
        "        )\n",
        "\n",
        "        df_split['emotion_names_raw'] = df_split['emotion_ids_list'].apply(\n",
        "            lambda ids: [GOEMOTIONS_ID_TO_LABEL[idx] for idx in ids if idx in GOEMOTIONS_ID_TO_LABEL]\n",
        "        )\n",
        "\n",
        "        df_split['emotion'] = df_split['emotion_names_raw'].apply(\n",
        "            lambda x: ['neutral'] if not x else x\n",
        "        )\n",
        "        # Assign 'neutral' where no other emotions are found\n",
        "        df_processed = df_split[['text', 'emotion']].copy()\n",
        "        print(f\"Processed {filepath.split('/')[-1]}. Final shape: {df_processed.shape}\")\n",
        "        return df_processed\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {filepath.split('/')[-1]} not found. Please ensure all 3 GoEmotions TSV files are in the specified Drive path.\")\n",
        "        return pd.DataFrame(columns=['text', 'emotion'])\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred loading or processing {filepath.split('/')[-1]}: {e}\")\n",
        "        return pd.DataFrame(columns=['text', 'emotion'])"
      ],
      "metadata": {
        "id": "xH-kNQ1UojF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Paths ---\n",
        "# IMPORTANT: Update these paths to where you've stored your data.\n",
        "# These paths are relative to where you run this script.\n",
        "# Ensure your 'data' folder is structured as:\n",
        "# data/\n",
        "# ├── go-emotions/\n",
        "# │   ├── train.tsv\n",
        "# │   ├── dev.tsv\n",
        "# │   └── test.tsv\n",
        "# └── synthetic/\n",
        "#     └── synthetic-emotion-data.csv (or your synthetic data file name)\n",
        "go_emotions_data_path = \"/content/drive/MyDrive/ML_Emotion_Classifier/data/go_emotions/\"\n",
        "go_emotions_train_filepath = f\"{go_emotions_data_path}train.tsv\"\n",
        "go_emotions_dev_filepath = f\"{go_emotions_data_path}dev.tsv\"\n",
        "go_emotions_test_filepath = f\"{go_emotions_data_path}test.tsv\"\n",
        "\n",
        "synthetic_data_filepath = \"/content/drive/MyDrive/ML_Emotion_Classifier/data/synthetic-emotion-data.csv\""
      ],
      "metadata": {
        "id": "3OIEtDtvd8FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Loading and processing official GoEmotions splits ---\")\n",
        "df_go_emotions_train = load_and_process_go_emotions_split(go_emotions_train_filepath)\n",
        "print(f\"Shape of df_go_emotions_train after processing: {df_go_emotions_train.shape}\")\n",
        "df_go_emotions_val = load_and_process_go_emotions_split(go_emotions_dev_filepath)\n",
        "df_go_emotions_test = load_and_process_go_emotions_split(go_emotions_test_filepath)\n",
        "print(\"--------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "sit5dzkmd_OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Save Path ---\n",
        "# IMPORTANT: Update this path to where you want to save your trained model.\n",
        "# This path is relative to where you run this script.\n",
        "MODEL_SAVE_PATH = \"./models/emo-tag/\""
      ],
      "metadata": {
        "id": "BasTurKCHR3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Loading synthetic data ---\")\n",
        "try:\n",
        "    df_synthetic = pd.read_csv(synthetic_data_filepath)\n",
        "    if 'emotion' in df_synthetic.columns:\n",
        "        df_synthetic['emotion'] = df_synthetic['emotion'].apply(parse_synthetic_emotion_string)\n",
        "        print(f\"Loaded and processed synthetic data. Shape: {df_synthetic.shape}\")\n",
        "    else:\n",
        "        print(f\"Error: 'emotion' column not found in synthetic data file: {synthetic_data_filepath}\")\n",
        "        print(f\"Available columns: {df_synthetic.columns.tolist()}\")\n",
        "        df_synthetic = pd.DataFrame(columns=['text', 'emotion']) # Create empty DF to prevent errors\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Synthetic data file not found at {synthetic_data_filepath}. Please check the path.\")\n",
        "    df_synthetic = pd.DataFrame(columns=['text', 'emotion']) # Create empty DF to prevent errors\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred loading or processing synthetic data: {e}\")\n",
        "    df_synthetic = pd.DataFrame(columns=['text', 'emotion']) # Create empty DF to prevent errors\n",
        "\n",
        "print(f\"Shape of df_synthetic after loading/processing: {df_synthetic.shape}\")\n",
        "print(\"--------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "ztbjr9K1eCfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Combine Datasets ---\n",
        "print(\"\\n--- Verifying column consistency before combining ---\")\n",
        "print(f\"df_go_emotions_train columns: {df_go_emotions_train.columns.tolist()}\")\n",
        "print(f\"df_synthetic columns: {df_synthetic.columns.tolist()}\")\n",
        "\n",
        "\n",
        "if not df_go_emotions_train.empty and not df_synthetic.empty and \\\n",
        "   'text' in df_go_emotions_train.columns and 'emotion' in df_go_emotions_train.columns and \\\n",
        "   'text' in df_synthetic.columns and 'emotion' in df_synthetic.columns:\n",
        "    print(\"Both GoEmotions train and synthetic dataframes have 'text' and 'emotion' columns.\")\n",
        "else:\n",
        "    print(\"Error: Column names mismatch or dataframes are empty. Check your data loading steps and synthetic data's column names.\")\n",
        "    print(\"Proceeding with potentially empty combined dataframe.\")\n",
        "\n",
        "    train_df = pd.DataFrame(columns=['text', 'emotion'])\n",
        "    val_df = pd.DataFrame(columns=['text', 'emotion'])\n",
        "    test_df = pd.DataFrame(columns=['text', 'emotion'])"
      ],
      "metadata": {
        "id": "VXjuVeRKo4nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Combining datasets ---\")\n",
        "if not df_go_emotions_train.empty:\n",
        "    train_df = pd.concat([df_go_emotions_train, df_synthetic], ignore_index=True)\n",
        "else:\n",
        "    print(\"df_go_emotions_train is empty. Cannot combine. Using empty train_df.\")\n",
        "    train_df = pd.DataFrame(columns=['text', 'emotion'])\n",
        "\n",
        "\n",
        "val_df = df_go_emotions_val # Validation set remains official dev data\n",
        "test_df = df_go_emotions_test # Test set remains official test data\n",
        "\n",
        "print(f\"Combined train_df shape: {train_df.shape}\")\n",
        "print(f\"Val_df shape: {val_df.shape}\")\n",
        "print(f\"Test_df shape: {test_df.shape}\")\n",
        "print(\"--------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "OOnXr9OheFc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Verify the combined dataset's emotion frequencies ---\n",
        "print(\"\\n--- Verifying Combined Train Dataset Emotion Frequencies ---\")\n",
        "mlb = MultiLabelBinarizer(classes=GOEMOTIONS_LABELS)\n",
        "\n",
        "if not train_df.empty:\n",
        "    labels_binary = mlb.fit_transform(train_df['emotion'])\n",
        "\n",
        "    df_labels_binary = pd.DataFrame(labels_binary, columns=mlb.classes_)\n",
        "\n",
        "    combined_train_counts = df_labels_binary.sum().sort_values(ascending=False)\n",
        "\n",
        "    print(\"\\nEmotion Frequencies in the COMBINED TRAIN dataset:\")\n",
        "    print(combined_train_counts)\n",
        "\n",
        "    plt.figure(figsize=(18, 9))\n",
        "    sns.barplot(x=combined_train_counts.index, y=combined_train_counts.values, palette='viridis')\n",
        "    plt.title('Emotion Frequencies: Combined Train Data (GoEmotions + Synthetic)', fontsize=18)\n",
        "    plt.xlabel('Emotion', fontsize=14)\n",
        "    plt.ylabel('Frequency', fontsize=14)\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Combined train_df is empty. Cannot calculate and visualize frequencies.\")\n",
        "print(\"--------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "JkaOX6P0pCYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Convert to Hugging Face Dataset ---\n",
        "print(\"\\n--- Converting to Hugging Face Dataset ---\")\n",
        "\n",
        "if not val_df.empty:\n",
        "    val_labels_binary = mlb.transform(val_df['emotion'])\n",
        "    val_hf_dataset = Dataset.from_pandas(pd.DataFrame({'text': val_df['text'], 'labels': val_labels_binary.tolist()}))\n",
        "else:\n",
        "    val_hf_dataset = Dataset.from_pandas(pd.DataFrame(columns=['text', 'labels']))\n",
        "\n",
        "if not test_df.empty:\n",
        "    test_labels_binary = mlb.transform(test_df['emotion'])\n",
        "    test_hf_dataset = Dataset.from_pandas(pd.DataFrame({'text': test_df['text'], 'labels': test_labels_binary.tolist()}))\n",
        "else:\n",
        "    test_hf_dataset = Dataset.from_pandas(pd.DataFrame(columns=['text', 'labels']))\n",
        "\n",
        "if not train_df.empty:\n",
        "    train_hf_dataset = Dataset.from_pandas(pd.DataFrame({'text': train_df['text'], 'labels': labels_binary.tolist()}))\n",
        "else:\n",
        "    train_hf_dataset = Dataset.from_pandas(pd.DataFrame(columns=['text', 'labels']))\n",
        "\n",
        "\n",
        "print(\"Hugging Face Datasets created.\")\n",
        "print(f\"train_hf_dataset columns: {train_hf_dataset.column_names}\")\n",
        "print(f\"val_hf_dataset columns: {val_hf_dataset.column_names}\")\n",
        "print(f\"test_hf_dataset columns: {test_hf_dataset.column_names}\")\n",
        "print(\"--------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "LzIh5kdPpGjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Undersampling of 'neutral' Class ---\n",
        "print(\"\\n--- Starting Undersampling of 'neutral' Class ---\")\n",
        "\n",
        "neutral_examples = []\n",
        "other_emotion_examples = []\n",
        "\n",
        "if not train_df.empty:\n",
        "    for index, row in train_df.iterrows():\n",
        "        if len(row['emotion']) == 1 and 'neutral' in row['emotion']:\n",
        "            neutral_examples.append(row)\n",
        "        else:\n",
        "            other_emotion_examples.append(row)\n",
        "\n",
        "    df_neutral_class = pd.DataFrame(neutral_examples)\n",
        "    df_other_emotions_class = pd.DataFrame(other_emotion_examples)\n",
        "\n",
        "    print(f\"Original 'neutral' count in train_df: {len(df_neutral_class)}\")\n",
        "    print(f\"Original other emotions count in train_df: {len(df_other_emotions_class)}\")"
      ],
      "metadata": {
        "id": "DhNw5A0PrEJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_neutral_count = 2750\n",
        "if len(df_neutral_class) > target_neutral_count:\n",
        "    df_neutral_undersampled = df_neutral_class.sample(n=target_neutral_count, random_state=42)\n",
        "    print(f\"Undersampled 'neutral' from {len(df_neutral_class)} to {len(df_neutral_undersampled)} examples.\")\n",
        "else:\n",
        "    df_neutral_undersampled = df_neutral_class\n",
        "    print(f\"'neutral' count ({len(df_neutral_class)}) is already at or below the target ({target_neutral_count}). No undersampling performed.\")\n",
        "\n",
        "train_df_undersampled = pd.concat([df_other_emotions_class, df_neutral_undersampled], ignore_index=True)\n",
        "\n",
        "print(f\"New combined (undersampled) train_df shape: {train_df_undersampled.shape}\")\n",
        "\n",
        "print(\"--- 'neutral' Undersampling Complete ---\")\n",
        "print(\"--------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "FFfqQ78KrIN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Verify Undersampled Train Dataset Emotion Frequencies ---\n",
        "print(\"\\n--- Verifying Undersampled Train Dataset Emotion Frequencies ---\")\n",
        "\n",
        "mlb_undersampled = MultiLabelBinarizer(classes=GOEMOTIONS_LABELS)"
      ],
      "metadata": {
        "id": "pK6JCkWtrXTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not train_df_undersampled.empty:\n",
        "    labels_binary_undersampled = mlb_undersampled.fit_transform(train_df_undersampled['emotion'])\n",
        "\n",
        "    df_labels_binary_undersampled = pd.DataFrame(labels_binary_undersampled, columns=mlb_undersampled.classes_)\n",
        "\n",
        "    combined_train_counts_undersampled = df_labels_binary_undersampled.sum().sort_values(ascending=False)\n",
        "\n",
        "    print(\"\\nEmotion Frequencies in the UNDERSAMPLED TRAIN dataset:\")\n",
        "    print(combined_train_counts_undersampled)\n",
        "\n",
        "    plt.figure(figsize=(18, 9))\n",
        "    sns.barplot(x=combined_train_counts_undersampled.index, y=combined_train_counts_undersampled.values, palette='viridis')\n",
        "    plt.title('Emotion Frequencies: Undersampled Train Data', fontsize=18)\n",
        "    plt.xlabel('Emotion', fontsize=14)\n",
        "    plt.ylabel('Frequency', fontsize=14)\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Undersampled train_df is empty. Cannot calculate and visualize frequencies.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Converting Undersampled Train Dataset to Hugging Face Dataset ---\")\n",
        "print(\"--------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "q9MCSIXUrZCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Convert Undersampled Train Dataset to Hugging Face Dataset ---\n",
        "if not train_df_undersampled.empty:\n",
        "    train_hf_dataset_undersampled = Dataset.from_pandas(\n",
        "        pd.DataFrame({'text': train_df_undersampled['text'], 'labels': labels_binary_undersampled.tolist()})\n",
        "    )\n",
        "    print(f\"train_hf_dataset_undersampled columns: {train_hf_dataset_undersampled.column_names}\")\n",
        "    print(f\"train_hf_dataset_undersampled size: {len(train_hf_dataset_undersampled)}\")\n",
        "else:\n",
        "    train_hf_dataset_undersampled = Dataset.from_pandas(pd.DataFrame(columns=['text', 'labels']))\n",
        "    print(\"train_hf_dataset_undersampled is empty.\")\n",
        "\n",
        "print(\"\\n--- Data Preparation for Training Complete ---\")\n",
        "print(\"--------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "Eh0MfJBqreIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Initialization ---\n",
        "print(\"\\n--- Initializing Model for Multi-label Classification ---\")\n",
        "\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "num_classes = len(GOEMOTIONS_LABELS)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_classes,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "print(f\"Model '{model_checkpoint}' loaded for multi-label classification with {num_classes} labels.\")\n",
        "print(\"--------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "9wfBW2_2RElm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Custom Loss Function (Focal Loss) ---\n",
        "def focal_loss_with_logits(inputs, targets, alpha=0.25, gamma=2.0, pos_weight=None):\n",
        "    bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none', pos_weight=pos_weight)\n",
        "    prob = torch.sigmoid(inputs)\n",
        "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
        "    alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "    focal_term = (1 - p_t) ** gamma\n",
        "    loss = alpha_t * focal_term * bce_loss\n",
        "    return loss.mean()\n",
        "print(\"Focal Loss function 'focal_loss_with_logits' defined.\")\n",
        "print(\"--------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "gqIrMWDQRKCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Custom Trainer for Loss Function Integration ---\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, *args, loss_weights=None, focal_alpha=0.25, focal_gamma=2.0, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_weights = loss_weights\n",
        "        self.focal_alpha = focal_alpha\n",
        "        self.focal_gamma = focal_gamma\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss_weights = loss_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        weights_on_device = self.loss_weights.to(logits.device)\n",
        "        loss = focal_loss_with_logits(\n",
        "            inputs=logits,\n",
        "            targets=labels.float(),\n",
        "            alpha=self.focal_alpha,\n",
        "            gamma=self.focal_gamma,\n",
        "            pos_weight=weights_on_device\n",
        "        )\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "print(f\"Model '{model_checkpoint}' loaded for multi-label classification with {num_classes} labels.\")\n",
        "print(\"A CustomTrainer is defined to incorporate class weighting into the loss function.\")\n",
        "print(\"-----------------------------------------\")"
      ],
      "metadata": {
        "id": "stq7txcoyH9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tokenizer and Dataset Tokenization ---\n",
        "print(\"\\n--- Initializing Tokenizer and Tokenizing Datasets ---\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    if 'text' not in examples:\n",
        "        raise ValueError(\"The 'text' column is missing from the dataset examples.\")\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "\n",
        "print(\"Tokenizing train dataset...\")\n",
        "tokenized_train_dataset = train_hf_dataset_undersampled.map(tokenize_function, batched=True)\n",
        "print(\"Tokenizing validation dataset...\")\n",
        "tokenized_val_dataset = val_hf_dataset.map(tokenize_function, batched=True)\n",
        "print(\"Tokenizing test dataset...\")\n",
        "tokenized_test_dataset = test_hf_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['text'])\n",
        "tokenized_val_dataset = tokenized_val_dataset.remove_columns(['text'])\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns(['text'])\n",
        "\n",
        "tokenized_train_dataset.set_format(\"torch\")\n",
        "tokenized_val_dataset.set_format(\"torch\")\n",
        "tokenized_test_dataset.set_format(\"torch\")\n",
        "\n",
        "print(\"Datasets tokenized and formatted for PyTorch.\")\n",
        "print(f\"Tokenized train dataset features: {tokenized_train_dataset.column_names}\")\n",
        "print(f\"Tokenized val dataset features: {tokenized_val_dataset.column_names}\")\n",
        "print(f\"Tokenized test dataset features: {tokenized_test_dataset.column_names}\")\n",
        "print(\"-----------------------------------------\")"
      ],
      "metadata": {
        "id": "S2WXX63PyDV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Calculate Class Weights ---\n",
        "print(\"\\n--- Calculating Class Weights ---\")\n",
        "labels_array = np.array(train_hf_dataset_undersampled['labels'])\n",
        "label_counts = np.sum(labels_array, axis=0)\n",
        "epsilon = 1e-6\n",
        "total_samples_in_train = len(train_hf_dataset_undersampled)\n",
        "num_classes = len(GOEMOTIONS_LABELS)\n",
        "\n",
        "weights = np.zeros(num_classes)\n",
        "for i, count in enumerate(label_counts):\n",
        "    weights[i] = total_samples_in_train / (num_classes * (count + epsilon))\n",
        "\n",
        "class_weights_tensor = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "print(\"Class Frequencies (from undersampled train_hf_dataset_undersampled):\")\n",
        "for i, label in enumerate(GOEMOTIONS_LABELS):\n",
        "    print(f\"- {label}: {label_counts[i]} (Calculated Weight: {class_weights_tensor[i]:.2f})\")\n",
        "\n",
        "print(\"-----------------------------------------\")"
      ],
      "metadata": {
        "id": "M0OGJV51x-6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Evaluation Metrics ---\n",
        "print(\"\\n--- Defining Evaluation Metrics ---\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    labels = p.label_ids\n",
        "    probabilities = 1 / (1 + np.exp(-predictions))\n",
        "    binary_predictions = (probabilities > 0.5).astype(int)\n",
        "    macro_f1 = f1_score(labels, binary_predictions, average='macro', zero_division=0)\n",
        "    macro_precision = precision_score(labels, binary_predictions, average='macro', zero_division=0)\n",
        "    macro_recall = recall_score(labels, binary_predictions, average='macro', zero_division=0)\n",
        "\n",
        "    micro_f1 = f1_score(labels, binary_predictions, average='micro', zero_division=0)\n",
        "    micro_precision = precision_score(labels, binary_predictions, average='micro', zero_division=0)\n",
        "    micro_recall = recall_score(labels, binary_predictions, average='micro', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        \"f1_macro\": macro_f1,\n",
        "        \"precision_macro\": macro_precision,\n",
        "        \"recall_macro\": macro_recall,\n",
        "        \"f1_micro\": micro_f1,\n",
        "        \"precision_micro\": micro_precision,\n",
        "        \"recall_micro\": micro_recall,\n",
        "    }\n",
        "\n",
        "print(\"Compute metrics function 'compute_metrics' defined (focusing on macro averages).\")\n",
        "print(\"-----------------------------------------\")"
      ],
      "metadata": {
        "id": "uxUJQVuPyMBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Arguments ---\n",
        "print(\"\\n--- Setting up Training Arguments ---\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/goemotions_model_v2\",\n",
        "    num_train_epochs=6,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.02,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=\"wandb\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "print(\"Training arguments defined.\")\n",
        "print(\"-----------------------------------------\")"
      ],
      "metadata": {
        "id": "uHU-YIBTyPWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Trainer Initialization and Training ---\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_threshold=0.001\n",
        ")\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "dZZV9Hrgoc9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    loss_weights=class_weights_tensor,\n",
        "    focal_alpha=0.25,\n",
        "    focal_gamma=2.0,\n",
        "    callbacks=[early_stopping_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "CzBM4tLczxrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(\"-----------------------------------------\")"
      ],
      "metadata": {
        "id": "xVDWqKskzzCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---Validation Predictions for Threshold Analysis---\n",
        "print(\"\\n--- Obtaining Validation Predictions for Threshold Analysis ---\")\n",
        "\n",
        "val_predictions_output = trainer.predict(tokenized_val_dataset)\n",
        "val_logits = val_predictions_output.predictions\n",
        "val_true = val_predictions_output.label_ids\n",
        "val_probs = 1 / (1 + np.exp(-val_logits))\n",
        "\n",
        "print(\"Validation predictions obtained for threshold analysis.\")\n",
        "\n",
        "thresholds = np.arange(0.05, 0.96, 0.05)\n",
        "\n",
        "macro_f1_scores = []\n",
        "micro_f1_scores = []\n",
        "macro_precision_scores = []\n",
        "micro_precision_scores = []\n",
        "macro_recall_scores = []\n",
        "micro_recall_scores = []\n",
        "\n",
        "print(\"Threshold | Macro F1 | Micro F1 | Macro P  | Micro P  | Macro R  | Micro R\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for t in thresholds:\n",
        "    val_preds = (val_probs > t).astype(int)\n",
        "\n",
        "    micro_f1 = f1_score(val_true, val_preds, average='micro', zero_division=0)\n",
        "    macro_f1 = f1_score(val_true, val_preds, average='macro', zero_division=0)\n",
        "    micro_p = precision_score(val_true, val_preds, average='micro', zero_division=0)\n",
        "    macro_p = precision_score(val_true, val_preds, average='macro', zero_division=0)\n",
        "    micro_r = recall_score(val_true, val_preds, average='micro', zero_division=0)\n",
        "    macro_r = recall_score(val_true, val_preds, average='macro', zero_division=0)\n",
        "\n",
        "    macro_f1_scores.append(macro_f1)\n",
        "    micro_f1_scores.append(micro_f1)\n",
        "    macro_precision_scores.append(macro_p)\n",
        "    micro_precision_scores.append(micro_p)\n",
        "    macro_recall_scores.append(macro_r)\n",
        "    micro_recall_scores.append(micro_r)\n",
        "\n",
        "    print(f\"{t:.2f}      | {macro_f1:.4f}   | {micro_f1:.4f}   | {macro_p:.4f}   | {micro_p:.4f}   | {macro_r:.4f}   | {micro_r:.4f}\")\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(thresholds, macro_f1_scores, label='Macro F1', marker='o')\n",
        "plt.plot(thresholds, micro_f1_scores, label='Micro F1', marker='o')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('F1 Score vs. Classification Threshold')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the threshold that maximizes Macro F1\n",
        "optimal_macro_f1_idx = np.argmax(macro_f1_scores)\n",
        "optimal_threshold_macro_f1 = thresholds[optimal_macro_f1_idx]\n",
        "print(f\"\\nOptimal Threshold for Macro F1: {optimal_threshold_macro_f1:.2f}\")\n",
        "print(f\"Max Macro F1 at this threshold: {macro_f1_scores[optimal_macro_f1_idx]:.4f}\")\n",
        "print(f\"Corresponding Micro F1: {micro_f1_scores[optimal_macro_f1_idx]:.4f}\")\n",
        "print(\"-----------------------------------------\")"
      ],
      "metadata": {
        "id": "G9Dq7GDHkfJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---Final Evaluation on Test Set---\n",
        "print(\"\\n--- Evaluating Model on Test Set ---\")\n",
        "final_test_results = trainer.evaluate(tokenized_test_dataset)\n",
        "print(f\"Final Test Set Evaluation Results: {final_test_results}\")\n",
        "\n",
        "# --- Add these lines to save the best model to Google Drive ---\n",
        "# Make sure Google Drive is mounted at /content/drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "#model_save_path = \"/content/drive/MyDrive/ML_Emotion_Classifier/emo_tag/\" # Consider a new path for this model\n",
        "#os.makedirs(model_save_path, exist_ok=True)\n",
        "#trainer.save_model(model_save_path) # This saves the model and tokenizer\n",
        "#print(f\"Best model saved to {model_save_path} directory in your Google Drive.\")\n",
        "# --- Final Evaluation on Test Set ---\n",
        "print(f\"Final Test Set Evaluation Results: {final_test_results}\")\n",
        "print(\"--------------------------------------------------------------------------------\")\n",
        "\n",
        "#---Detailed classification report on Test Set---\n",
        "print(\"\\n--- Generating detailed classification report on Test Set ---\")\n",
        "predictions_output = trainer.predict(tokenized_test_dataset)\n",
        "logits = predictions_output.predictions\n",
        "true_labels = predictions_output.label_ids\n",
        "optimal_threshold = 0.10 #\n",
        "probabilities = 1 / (1 + np.exp(-logits))\n",
        "predicted_labels = (probabilities >= optimal_threshold).astype(int)\n",
        "\n",
        "report = classification_report(true_labels, predicted_labels,\n",
        "                               target_names=GOEMOTIONS_LABELS,\n",
        "                               zero_division='warn')\n",
        "print(report)\n",
        "print(\"\\n--- Model Training and Evaluation Flow Complete ---\")\n",
        "print(\"---------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "FXngVE90Ci32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c5649d1"
      },
      "source": [
        "# --- Define the prediction function here ---\n",
        "def predict_emotions(text, model, tokenizer, labels, threshold=0.5):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
        "    inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    probabilities = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
        "    prob_dict = dict(zip(labels, probabilities))\n",
        "    predicted_labels = [label for label, prob in prob_dict.items() if prob >= threshold]\n",
        "\n",
        "    return predicted_labels, prob_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test Questions Post-Training ---\n",
        "print(\"\\n--- Testing Model with Custom Inputs ---\")\n",
        "\n",
        "trained_model = model\n",
        "trained_tokenizer = tokenizer\n",
        "\n",
        "GOEMOTIONS_LABELS = [\n",
        "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
        "    'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n",
        "    'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',\n",
        "    'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
        "]\n",
        "\n",
        "trained_model.config.id2label = {i: label for i, label in enumerate(GOEMOTIONS_LABELS)}\n",
        "trained_model.config.label2id = {label: i for i, label in enumerate(GOEMOTIONS_LABELS)}\n",
        "\n",
        "if trained_model and trained_tokenizer:\n",
        "    model_labels = list(trained_model.config.id2label.values())\n",
        "\n",
        "    # Scenario 1: A complex emotional text\n",
        "    text1 = \"I am so incredibly happy with this result, it truly fills me with joy and a sense of pride!\"\n",
        "    predicted_labels1, probs1 = predict_emotions(text1, trained_model, trained_tokenizer, model_labels, threshold=optimal_threshold_macro_f1)\n",
        "    print(\"\\n--- Test Scenario 1 ---\")\n",
        "    print(f\"Text: '{text1}'\")\n",
        "    print(f\"Predicted Emotions: {predicted_labels1}\")\n",
        "    sorted_probs1 = sorted(probs1.items(), key=lambda item: item[1], reverse=True)\n",
        "    print(f\"Top Probabilities: {sorted_probs1[:5]}\") # Print top 5 probabilities\n",
        "\n",
        "    # Scenario 2: A seemingly neutral text that might have subtle underlying emotion or be truly neutral\n",
        "    text2 = \"The sky is blue today. It's a typical Tuesday afternoon.\"\n",
        "    predicted_labels2, probs2 = predict_emotions(text2, trained_model, trained_tokenizer, model_labels, threshold=optimal_threshold_macro_f1)\n",
        "    print(\"\\n--- Test Scenario 2 ---\")\n",
        "    print(f\"Text: '{text2}'\")\n",
        "    print(f\"Predicted Emotions: {predicted_labels2}\")\n",
        "    sorted_probs2 = sorted(probs2.items(), key=lambda item: item[1], reverse=True)\n",
        "    print(f\"Top Probabilities: {sorted_probs2[:5]}\")\n",
        "\n",
        "   # Scenario 3: A text with a negative or mixed emotion\n",
        "    text3 = \"I'm quite annoyed by the constant noise, but also a bit sad it has come to this.\"\n",
        "    predicted_labels3, probs3 = predict_emotions(text3, trained_model, trained_tokenizer, model_labels, threshold=optimal_threshold_macro_f1)\n",
        "    print(\"\\n--- Test Scenario 3 ---\")\n",
        "    print(f\"Text: '{text3}'\")\n",
        "    print(f\"Predicted Emotions: {predicted_labels3}\")\n",
        "    sorted_probs3 = sorted(probs3.items(), key=lambda item: item[1], reverse=True)\n",
        "    print(f\"Top Probabilities: {sorted_probs3[:5]}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping prediction examples as model/tokenizer are not yet ready.\")\n",
        "\n",
        "print(\"\\n--- Model Training, Evaluation, and Custom Prediction Flow Complete ---\")\n",
        "print(\"---------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "JSabvIZhCho_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}